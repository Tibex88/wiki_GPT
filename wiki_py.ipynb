{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "2rbSQpU2-C0I"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tibex88/wiki_GPT/blob/main/wiki_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install dependancies"
      ],
      "metadata": {
        "id": "E35GzIYBjO5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####WIKIPEDIA API\n",
        "\n"
      ],
      "metadata": {
        "id": "ap5ENU5E9IAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "id": "qLmcXO2jeNDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####FLASH AND NGROK"
      ],
      "metadata": {
        "id": "_S-yPrX59PhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install pyngrok==4.1.1\n",
        "!ngrok authtoken 2MBCLllCx6vqHIh1DgyvGTs60iK_7JitdzfDGUU8khCWtopq3"
      ],
      "metadata": {
        "id": "dLa2hiOmjfw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####CHANGE ENCODING FOR THE FF DEPENDANCIES\n"
      ],
      "metadata": {
        "id": "GBAnB3YO9W53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "encoding = locale.getpreferredencoding()\n",
        "if encoding != \"UTF-8\":\n",
        "  locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "  !ls\n",
        "locale.getpreferredencoding()"
      ],
      "metadata": {
        "id": "hEGJb-rw7A3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask_cors"
      ],
      "metadata": {
        "id": "u93hLVLw80pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "id": "bJZM4mRN9FCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client"
      ],
      "metadata": {
        "id": "1NjBp_cjYdFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index sentence_transformers"
      ],
      "metadata": {
        "id": "d_lAZwo3Lg2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi as wapi"
      ],
      "metadata": {
        "id": "MDS07ETGX9Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "SPII4JKn3S6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get text from articles"
      ],
      "metadata": {
        "id": "yz_ullpbjj3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "user_agent = 'Mozilla/5.0 (iPhone; CPU iPhone OS 5_1 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9B179 Safari/7534.48.3'\n",
        "\n",
        "def get_wikipedia_article(title):\n",
        "    wiki_wiki = wapi.Wikipedia(user_agent = user_agent,language='en', extract_format= wapi.ExtractFormat.WIKI)\n",
        "    page = wiki_wiki.page(title)\n",
        "\n",
        "\n",
        "    if not page.exists():\n",
        "        return None\n",
        "    text = page.text\n",
        "    return text"
      ],
      "metadata": {
        "id": "cMRMP54KmOKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose Article"
      ],
      "metadata": {
        "id": "FAlzmbonjs4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "article_title = 'USS Marmora (1862)'\n",
        "# summary, sections = get_wikipedia_article(article_title)\n",
        "article = get_wikipedia_article(article_title)\n",
        "# print(article)\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,           # Usually chunk sizes are much larger than this\n",
        "    chunk_overlap  = 20,        # Overlap is needed incase the text is split in odd places\n",
        "    length_function = len,\n",
        ")\n",
        "text = text_splitter.split_text(article)\n",
        "# print((text))"
      ],
      "metadata": {
        "id": "7x-Kmci8nv5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSizP03C8pZq"
      },
      "source": [
        "### Define the embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mT0mMqtctGl"
      },
      "outputs": [],
      "source": [
        "# from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
        "from llama_index import LangchainEmbedding\n",
        "\n",
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embed texts from chunks\n"
      ],
      "metadata": {
        "id": "-LgKY7cXljQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "\n",
        "for i in text:\n",
        "    section_embedding = embed_model.get_text_embedding(i)\n",
        "    embeddings.append(section_embedding)\n",
        "    # print(section_embedding)"
      ],
      "metadata": {
        "id": "qF9fdDzploSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check dimensions from the embedding"
      ],
      "metadata": {
        "id": "2rbSQpU2-C0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dimension = embed_model.get_text_embedding(\"hello\")\n",
        "len(dimension)"
      ],
      "metadata": {
        "id": "eomA73CHmhYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExwpoDsP85DP"
      },
      "source": [
        "### Connect to Pinecone as a vector database store and upsert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvil07GHbzCF"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "\n",
        "# initialize connection to pinecone\n",
        "pinecone.init(\n",
        "    api_key= 'f0048841-1886-4d17-bffd-a5fe39c183a9',\n",
        "    environment= 'asia-southeast1-gcp-free'\n",
        ")\n",
        "\n",
        "# create the index if it does not exist already\n",
        "index_name = 'wiki'\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=len(dimension),\n",
        "        metric='cosine'\n",
        "    )\n",
        "\n",
        "# connect to the index\n",
        "pinecone_index = pinecone.Index(index_name)\n",
        "docs = []\n",
        "for idx, i in enumerate(text):\n",
        "  docs.append((\n",
        "        str(idx),\n",
        "        embeddings[idx],\n",
        "        {'text': i},\n",
        "    ))\n",
        "\n",
        "pinecone_index.upsert(vectors= docs, show_progress = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig #, LLaMATokenizer, LLaMAForCausalLM"
      ],
      "metadata": {
        "id": "pGgbiC_ePY_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Login to Huggingface"
      ],
      "metadata": {
        "id": "2YVX-ok-lC-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "fmwtQhVw8fWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#token for huggingface\n",
        "# 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"
      ],
      "metadata": {
        "id": "EjDx4gfaJT-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define model and tokenizer"
      ],
      "metadata": {
        "id": "aGMVXPLwlHbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                          use_auth_token=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                             device_map='auto',\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             use_auth_token=True,\n",
        "                                             load_in_4bit=True\n",
        "                                             )"
      ],
      "metadata": {
        "id": "LZdsO7ciPaek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get from db"
      ],
      "metadata": {
        "id": "rKBjtslSjeIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_from_db(query):\n",
        "    # embed query\n",
        "    xq = embed_model.get_text_embedding(query)\n",
        "    # query database\n",
        "    result = pinecone_index.query(xq, top_k=20, includeMetadata=True)\n",
        "    matches = []\n",
        "    for i in result['matches']:\n",
        "      # filter by score\n",
        "      if (i['score'] > 0.6):\n",
        "        print(i.metadata['text'])\n",
        "        # append best results\n",
        "        matches.append(i.metadata['text'])\n",
        "    return str(matches)"
      ],
      "metadata": {
        "id": "ZrWG0RUrjc4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prep and wrap instruction in prompt"
      ],
      "metadata": {
        "id": "kSvKslixfYaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "  You will be given texts related to a certain topic. Write a summary response that answers the question based on what is discussed in the texts.\n",
        "  Do not mention anything outside of what is provided. Don't answer anything outside the context you are provided.\n",
        "  If there isn't enough context, simply reply \"This topic was not discussed previously\"\n",
        "  \"\"\"\n",
        "\n",
        "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT + E_SYS\n",
        "\n",
        "def get_prompt(instruction):\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template"
      ],
      "metadata": {
        "id": "ukYOH2JzT9Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format context long with the query"
      ],
      "metadata": {
        "id": "ZFDruDN8-xPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(query, context):\n",
        "    return '''\n",
        "    ### Texts:\n",
        "    {context}\n",
        "\n",
        "    ### Question:\n",
        "    {query}\n",
        "    '''.format(context=context, query=query)"
      ],
      "metadata": {
        "id": "fzpbLtq0Kuc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generate output per prompt"
      ],
      "metadata": {
        "id": "YGOQXP9t-5p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt_template):\n",
        "    output = \"\"\n",
        "    inputs = tokenizer(\n",
        "        prompt_template,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,\n",
        "    )\n",
        "    print(\"Generating...\")\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=4028,\n",
        "    )\n",
        "    for s in generation_output.sequences:\n",
        "        output += tokenizer.decode(s)\n",
        "    return output"
      ],
      "metadata": {
        "id": "RlYaVuXrQAUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that runs the model\n",
        "def answer(query):\n",
        "    context = get_data_from_db(query)\n",
        "\n",
        "    prompt = format_prompt(query, context)\n",
        "    prompt_template = get_prompt(prompt)\n",
        "    return generate(prompt_template)"
      ],
      "metadata": {
        "id": "L2ucVHE1N8Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List of possible queries to test the model\n",
        "- when was Marmora built?\n",
        "- when was the USS Marmora built?\n",
        "- which ship captain was it built for?\n",
        "- what was the other ship that he operated?"
      ],
      "metadata": {
        "id": "juLiBW3CSILe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer(\"When was Marmora built?\")"
      ],
      "metadata": {
        "id": "NudFX-K0jsdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make the app available as an API\n"
      ],
      "metadata": {
        "id": "W6b_YKXC_d6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask_cors import CORS\n",
        "from flask import Flask, request\n",
        "\n",
        "\n",
        "#Running the flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "#lallow all origins access\n",
        "CORS(app, origins=['*'])\n",
        "\n",
        "#We meed to start ngrok when the app is run\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/\", methods=['GET'])\n",
        "def index():\n",
        "  # extract query param\n",
        "  query = request.args['query']\n",
        "  if query:\n",
        "    print(\"query\" ,query)\n",
        "    # pass query into the model\n",
        "    ans = answer(query)\n",
        "    return ans\n",
        "  else:\n",
        "    # reply for when no query in the params\n",
        "    return \"<h1>No valid response</p>\"\n",
        "\n",
        "# test route\n",
        "@app.route(\"/test\", methods=['GET'])\n",
        "def test_page():\n",
        "  return \"<h1>This is the test page!</hl>\"\n",
        "\n",
        "\n",
        "app.run()"
      ],
      "metadata": {
        "id": "hVbSbA71jkKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zRpFPqO5OcjX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}